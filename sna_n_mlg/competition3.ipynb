{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuYRQ_XwVGdv"
      },
      "source": [
        "# Competition 3. Link prediction\n",
        "\n",
        "## Challenge Overview\n",
        "The dataset contains publications that is described by a feature vector of the paper description. The dataset can be represented as a directed graph where nodes are publications and edges are citations. The random part of edges are dropped and also disconnected pairs of nodes are selected in the same amount as dropped edges. Let us denote dropped edges by label 1 and additionally selected pairs by label 0.\n",
        "\n",
        "The dataset is represented by 3 files:\n",
        "- node_feat.txt contains description of the papers in the format: _<32d feature vector>_\n",
        "-train_edges.txt contains existing edges in the format:\n",
        "_\\<node id\\> \\<node id\\>_\n",
        "-unlabeled_edges.txt contains unlabeled pairs of nodes in the format:\n",
        "_\\<node id\\> \\<node id\\>_\n",
        "\n",
        "Your task is to predict labels for unlabeled pairs of nodes: 0 — disconnected, 1 — connected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpjXvgQKVvWU"
      },
      "source": [
        "## Evaluation Criteria\n",
        "Here are balanced classes, so the usual accuracy metric is used:\n",
        "\n",
        "**Accuracy = True predictions / All predictions**\n",
        "\n",
        "The baselines are calculated as follows: \n",
        "1. Sample negative edges. Concatenate node features into edge features. Train Gradient boosting on edges features. Calculate score.\n",
        "2. Create a graph on existing edges. Train Laplacian Eigenmaps. Sample negative edges. Concatenate node embeddings into edge embeddings. Train Gradient boosting on edge embeddings. Calculate score.\n",
        "3. Create a graph using existing edges. Sample negative edges. Train a neural network consisting of GCN for node emedding and MLP for link prediction that maps Hadamart product edge embeddings into probability of edge existing. Calculate score.\n",
        "\n",
        "Baseline for grade 4: beat a minimum score\n",
        "\n",
        "Baseline for grade 6: beat a mean score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Ll32v8WGDP"
      },
      "source": [
        "## Решение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGFmVyQz3wsi",
        "outputId": "88f37af4-4c78-46d8-84ce-2d28e632af5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.7/dist-packages (0.8.0.post2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl -f https://data.dgl.ai/wheels/repo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCCDBmAaycsy",
        "outputId": "9c85b12d-0410-403f-9f7e-b99ffc75a5d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import copy\n",
        "import itertools\n",
        "import requests\n",
        "\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn import GraphConv, GATConv, SAGEConv\n",
        "from sklearn.decomposition import TruncatedSVD    \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj-Oz0ytWMqV"
      },
      "source": [
        "Скачаем данные и проанализируем их"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OZoy4IoAyiUe"
      },
      "outputs": [],
      "source": [
        "for name in ('node_feat.txt', 'train_edges.txt', 'unlabeled_edges.txt'):\n",
        "    url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/lp_comp/' + name\n",
        "    open(name, 'wb').write(requests.get(url).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MudXCkebz51_"
      },
      "outputs": [],
      "source": [
        "train_edges = np.loadtxt('train_edges.txt', dtype=np.int64)\n",
        "node_feat = np.loadtxt('node_feat.txt', dtype=np.float64)\n",
        "unlabeled_edges = np.loadtxt('unlabeled_edges.txt', dtype=np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEAp76PnCdd-",
        "outputId": "c1ebc829-0a6e-42e1-8c21-8db6b84bf1a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14322, 2), (12588, 32), (44014, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_edges.shape, node_feat.shape, unlabeled_edges.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_4QsMpxFSjP",
        "outputId": "fd733141-b3d1-42a5-f9b2-ad234b5ba85b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12587, 12587)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_edges.max(), unlabeled_edges.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR2Xz7ZzFXjE"
      },
      "source": [
        "В графе 12588 вершин (от 0 до 12587)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7A0Xzbj7GYKr"
      },
      "outputs": [],
      "source": [
        "NUM_NODES = 12588"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auUIeRS44XNy"
      },
      "source": [
        "Будем строить стекинг градиентных бустингов с разными способами создания эмбедингов ребер (Adamar, mean, concat, L1, L2, weighted_L1, weighted_L2) из эмбедингов вершин, которые построим как конкатенацию фичей вершин с ее эмбедингом через SVD разложении матрицы смежности графа."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xriIzcjJj5V1"
      },
      "outputs": [],
      "source": [
        "seed_array = (0, 1, 42)\n",
        "predictions = {\n",
        "    'weighted_L2': [],\n",
        "    'weighted_L1': [],\n",
        "    'Adamar': [],\n",
        "    'concat': [],\n",
        "    'L1': [],\n",
        "    'L2': [],\n",
        "    'mean': []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "px6-vgusmmzq"
      },
      "outputs": [],
      "source": [
        "def SVD_features_GBC(train_edges=train_edges,\n",
        "                     unlabeled_edges=unlabeled_edges,\n",
        "                     test_size=0.1,\n",
        "                     seed=42,\n",
        "                     node_feat=node_feat,\n",
        "                     n_components=128,\n",
        "                     n_estimators=1000,\n",
        "                     emb_edges_type='Adamar',\n",
        "                     emb_A=None,\n",
        "                     G=None):\n",
        "                  \n",
        "    assert emb_edges_type in (\n",
        "        'Adamar', 'concat', 'L1', 'L2',\n",
        "        'mean', 'weighted_L1', 'weighted_L2'\n",
        "        ), \"Incorrect emb_edges_type\"\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if emb_A is None:\n",
        "        G = nx.Graph()\n",
        "        G.add_nodes_from(np.arange(NUM_NODES))\n",
        "        G.add_edges_from(train_edges)\n",
        "        A = nx.to_numpy_array(G)\n",
        "        emb_A = TruncatedSVD(n_components=n_components).fit_transform(A)\n",
        "\n",
        "    u, v = train_edges[:, 0], train_edges[:, 1]\n",
        "    neg_v = np.random.randint(NUM_NODES, size=len(train_edges))\n",
        "    \n",
        "    emb = np.concatenate([node_feat, emb_A], axis=1)\n",
        "\n",
        "    if emb_edges_type == 'Adamar':\n",
        "        pos_X = emb[u] * emb[v]\n",
        "        neg_X = emb[u] * emb[neg_v]\n",
        "        X_test = emb[unlabeled_edges[:, 0]] * emb[unlabeled_edges[:, 1]]\n",
        "    elif emb_edges_type == 'concat':\n",
        "        pos_X = np.concatenate([emb[u], emb[v]], axis=1)\n",
        "        neg_X = np.concatenate([emb[u], emb[neg_v]], axis=1)\n",
        "        X_test = np.concatenate([emb[unlabeled_edges[:, 0]], emb[unlabeled_edges[:, 1]]], axis=1)\n",
        "    elif emb_edges_type == 'L1':\n",
        "        pos_X = np.abs(emb[u] - emb[v])\n",
        "        neg_X = np.abs(emb[u] - emb[neg_v])\n",
        "        X_test = np.abs(emb[unlabeled_edges[:, 0]] - emb[unlabeled_edges[:, 1]])\n",
        "    elif emb_edges_type == 'L2':\n",
        "        pos_X = (emb[u] - emb[v]) ** 2\n",
        "        neg_X = (emb[u] - emb[neg_v]) ** 2\n",
        "        X_test = (emb[unlabeled_edges[:, 0]] - emb[unlabeled_edges[:, 1]]) ** 2\n",
        "    elif emb_edges_type == 'mean':\n",
        "        pos_X = (emb[u] + emb[v]) / 2\n",
        "        neg_X = (emb[u] + emb[neg_v]) / 2\n",
        "        X_test = (emb[unlabeled_edges[:, 0]] + emb[unlabeled_edges[:, 1]]) / 2\n",
        "    elif emb_edges_type == 'weighted_L1':\n",
        "        pos_X, neg_X, X_test = [], [], []\n",
        "        for u_node, v_node, neg_v_node in zip(u, v, neg_v):\n",
        "            u_neigh = (emb[[u_node] + list(G.neighbors(u_node))]).mean(axis=0)\n",
        "            v_neigh = (emb[[v_node] + list(G.neighbors(v_node))]).mean(axis=0)\n",
        "            neg_v_neigh = (emb[[neg_v_node] + list(G.neighbors(neg_v_node))]).mean(axis=0)\n",
        "            pos_X.append(np.abs(u_neigh - v_neigh))\n",
        "            neg_X.append(np.abs(u_neigh - neg_v_neigh))\n",
        "        for u_node, v_node in unlabeled_edges:\n",
        "            u_neigh = (emb[[u_node] + list(G.neighbors(u_node))]).mean(axis=0)\n",
        "            v_neigh = (emb[[v_node] + list(G.neighbors(v_node))]).mean(axis=0)\n",
        "            X_test.append(np.abs(u_neigh - v_neigh))\n",
        "        pos_X, neg_X, X_test = np.array(pos_X), np.array(neg_X), np.array(X_test)\n",
        "    elif emb_edges_type == 'weighted_L2':\n",
        "        pos_X, neg_X, X_test = [], [], []\n",
        "        for u_node, v_node, neg_v_node in zip(u, v, neg_v):\n",
        "            u_neigh = (emb[[u_node] + list(G.neighbors(u_node))]).mean(axis=0)\n",
        "            v_neigh = (emb[[v_node] + list(G.neighbors(v_node))]).mean(axis=0)\n",
        "            neg_v_neigh = (emb[[neg_v_node] + list(G.neighbors(neg_v_node))]).mean(axis=0)\n",
        "            pos_X.append((u_neigh - v_neigh) ** 2)\n",
        "            neg_X.append((u_neigh - neg_v_neigh) ** 2)\n",
        "        for u_node, v_node in unlabeled_edges:\n",
        "            u_neigh = (emb[[u_node] + list(G.neighbors(u_node))]).mean(axis=0)\n",
        "            v_neigh = (emb[[v_node] + list(G.neighbors(v_node))]).mean(axis=0)\n",
        "            X_test.append((u_neigh - v_neigh) ** 2)\n",
        "        pos_X, neg_X, X_test = np.array(pos_X), np.array(neg_X), np.array(X_test)\n",
        "            \n",
        "    X = np.concatenate([pos_X, neg_X])\n",
        "    y = np.concatenate([np.ones(pos_X.shape[0]), np.zeros(neg_X.shape[0])])\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
        "    \n",
        "    gbc = XGBClassifier(n_estimators=n_estimators, random_state=seed).fit(X_train, y_train)\n",
        "    val_accuracy = accuracy_score(gbc.predict(X_val), y_val)\n",
        "    test_prediction = gbc.predict(X_test)\n",
        "\n",
        "    return val_accuracy, test_prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.Graph()\n",
        "G.add_nodes_from(np.arange(NUM_NODES))\n",
        "G.add_edges_from(train_edges)\n",
        "A = nx.to_numpy_array(G)\n",
        "emb_A = TruncatedSVD(n_components=256).fit_transform(A)"
      ],
      "metadata": {
        "id": "E-i8RS19jyIz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O9nctNdIFIO",
        "outputId": "962a712b-8bfe-4b7b-dc53-7a7ecbc132d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed = 0, \t type = weighted_L2, \t val_accuracy = 0.9790940766550522\n",
            "seed = 0, \t type = weighted_L1, \t val_accuracy = 0.9790940766550522\n",
            "seed = 0, \t type = Adamar, \t val_accuracy = 0.9616724738675958\n",
            "seed = 0, \t type = concat, \t val_accuracy = 0.9825783972125436\n",
            "seed = 0, \t type = L1, \t val_accuracy = 0.867595818815331\n",
            "seed = 0, \t type = L2, \t val_accuracy = 0.8536585365853658\n",
            "seed = 0, \t type = mean, \t val_accuracy = 0.8641114982578397\n",
            "seed = 1, \t type = weighted_L2, \t val_accuracy = 0.9790940766550522\n",
            "seed = 1, \t type = weighted_L1, \t val_accuracy = 0.9825783972125436\n",
            "seed = 1, \t type = Adamar, \t val_accuracy = 0.9721254355400697\n",
            "seed = 1, \t type = concat, \t val_accuracy = 0.975609756097561\n",
            "seed = 1, \t type = L1, \t val_accuracy = 0.8571428571428571\n",
            "seed = 1, \t type = L2, \t val_accuracy = 0.8571428571428571\n",
            "seed = 1, \t type = mean, \t val_accuracy = 0.8362369337979094\n",
            "seed = 42, \t type = weighted_L2, \t val_accuracy = 0.9651567944250871\n",
            "seed = 42, \t type = weighted_L1, \t val_accuracy = 0.9651567944250871\n",
            "seed = 42, \t type = Adamar, \t val_accuracy = 0.9790940766550522\n",
            "seed = 42, \t type = concat, \t val_accuracy = 0.975609756097561\n",
            "seed = 42, \t type = L1, \t val_accuracy = 0.8153310104529616\n",
            "seed = 42, \t type = L2, \t val_accuracy = 0.8292682926829268\n",
            "seed = 42, \t type = mean, \t val_accuracy = 0.8362369337979094\n"
          ]
        }
      ],
      "source": [
        "for seed in seed_array:\n",
        "    for emb_edges_type in predictions.keys():\n",
        "        accuracy, test_prediction = SVD_features_GBC(\n",
        "            emb_edges_type=emb_edges_type,\n",
        "            emb_A=emb_A,\n",
        "            seed=seed,\n",
        "            G=G,\n",
        "            n_estimators=1000,\n",
        "            test_size=0.01)\n",
        "        print(f'seed = {seed}, \\t type = {emb_edges_type}, \\t val_accuracy = {accuracy}')\n",
        "        predictions[emb_edges_type].append(test_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSkv7Gxe-5_L",
        "outputId": "910f00e0-3f41-4cf6-c417-dc9d051d0c1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Adamar': [array([0., 0., 0., ..., 1., 0., 0.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 0.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 1.])],\n",
              " 'L1': [array([0., 0., 0., ..., 0., 0., 1.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 1.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 1.])],\n",
              " 'L2': [array([0., 0., 0., ..., 0., 0., 1.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 1.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 1.])],\n",
              " 'concat': [array([0., 0., 0., ..., 1., 0., 0.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 0.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 0.])],\n",
              " 'mean': [array([0., 0., 0., ..., 1., 0., 1.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 1.]),\n",
              "  array([0., 0., 0., ..., 1., 0., 1.])],\n",
              " 'weighted_L1': [array([0., 0., 0., ..., 0., 0., 0.]),\n",
              "  array([0., 0., 0., ..., 0., 0., 1.]),\n",
              "  array([0., 0., 0., ..., 0., 0., 1.])],\n",
              " 'weighted_L2': [array([0., 0., 0., ..., 0., 0., 0.]),\n",
              "  array([0., 0., 0., ..., 0., 0., 1.]),\n",
              "  array([0., 0., 0., ..., 0., 0., 1.])]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pred = []\n",
        "for name, pr in predictions.items():\n",
        "    for arr in pr:\n",
        "        all_pred.append(arr)"
      ],
      "metadata": {
        "id": "oakQBZ2muFkS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlL3cZC1uHpE",
        "outputId": "38e856df-3347-4978-e4a7-553b4b488368"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0., 0., 0., ..., 0., 0., 0.]),\n",
              " array([0., 0., 0., ..., 0., 0., 1.]),\n",
              " array([0., 0., 0., ..., 0., 0., 1.]),\n",
              " array([0., 0., 0., ..., 0., 0., 0.]),\n",
              " array([0., 0., 0., ..., 0., 0., 1.]),\n",
              " array([0., 0., 0., ..., 0., 0., 1.]),\n",
              " array([0., 0., 0., ..., 1., 0., 0.]),\n",
              " array([0., 0., 0., ..., 1., 0., 0.]),\n",
              " array([0., 0., 0., ..., 1., 0., 1.]),\n",
              " array([0., 0., 0., ..., 1., 0., 0.]),\n",
              " array([0., 0., 0., ..., 1., 0., 0.]),\n",
              " array([0., 0., 0., ..., 1., 0., 0.]),\n",
              " array([0., 0., 0., ..., 0., 0., 1.]),\n",
              " array([0., 0., 0., ..., 1., 0., 1.]),\n",
              " array([0., 0., 0., ..., 1., 0., 1.]),\n",
              " array([0., 0., 0., ..., 0., 0., 1.]),\n",
              " array([0., 0., 0., ..., 1., 0., 1.]),\n",
              " array([0., 0., 0., ..., 1., 0., 1.]),\n",
              " array([0., 0., 0., ..., 1., 0., 1.]),\n",
              " array([0., 0., 0., ..., 1., 0., 1.]),\n",
              " array([0., 0., 0., ..., 1., 0., 1.])]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gbc_mean_predict = np.array(np.array(all_pred).mean(axis=0) > 0.5, dtype=np.int64)\n",
        "gbc_mean_predict.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "decY_DGDuH_3",
        "outputId": "55351ff3-d5b1-4758-ba9a-6c3f70d1d26d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10614"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gbc_mean_predict = np.array(np.array(all_pred).mean(axis=0) > 0, dtype=np.int64)\n",
        "gbc_mean_predict.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTk2eGPguLvc",
        "outputId": "d61c2e92-ac60-4d4a-e6a8-f60105ad5b0f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26741"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pred = []\n",
        "for name, pr in predictions.items():\n",
        "    if name in ('Adamar', 'concat', 'weighted_L1', 'weighted_L2'):\n",
        "        for arr in pr:\n",
        "            all_pred.append(arr)"
      ],
      "metadata": {
        "id": "B00b92CCuObL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbc_mean_predict = np.array(np.array(all_pred).mean(axis=0) > 0.5, dtype=np.int64)\n",
        "gbc_mean_predict.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTxjfSi9uaXw",
        "outputId": "41855d24-bcf7-4e9f-880d-86407e6c9d88"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7722"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем строить стэкинг по такому принципу: если хоть ОДНА модель посчитала, что ребро в графе есть, значит считаем что это ребро есть"
      ],
      "metadata": {
        "id": "DQx5MSd4XQr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbc_mean_predict = np.array(np.array(all_pred).mean(axis=0) > 0, dtype=np.int64)\n",
        "gbc_mean_predict.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0btNRUiubgy",
        "outputId": "04d4ee55-6581-480b-9955-d00ef42cbebb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18930"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nbAeh8Z3NQfG"
      },
      "outputs": [],
      "source": [
        "np.savetxt('submit_gbc.txt', gbc_mean_predict, fmt='%i', delimiter='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скор на тестовых данных: 0.8041"
      ],
      "metadata": {
        "id": "K6_0B_FkXgFk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK3_JRpm6tqv"
      },
      "source": [
        "Можно было бы в стекинг добавить нейронные сети, однако не удалось добиться на них качества больше чем 60%. Если подкрутить гиперпараметры или как-то по другому проводить само обучение, скорее всего качество бы повысилось. Ниже находятся неудачные эксперименты с ними"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0SdfO3_i6xMb"
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats):\n",
        "        super().__init__()\n",
        "        self.conv1 = GraphConv(in_feats, h_feats)\n",
        "        self.conv2 = GraphConv(h_feats, h_feats)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(in_feats, h_feats, 8, feat_drop=0.2)\n",
        "        self.conv2 = GATConv(h_feats * 8, h_feats, 1)\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = h.view(-1, h.size(1) * h.size(2))\n",
        "        h = F.elu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h.squeeze()\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
        "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
        "\n",
        "    def forward(self, g, in_feat):\n",
        "        h = self.conv1(g, in_feat)\n",
        "        h = F.relu(h)\n",
        "        h = self.conv2(g, h)\n",
        "        return h\n",
        "\n",
        "class MLPPredictor(nn.Module):\n",
        "    def __init__(self, h_feats):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
        "        self.W2 = nn.Linear(h_feats, 1)\n",
        "\n",
        "    def apply_edges(self, edges):\n",
        "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
        "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
        "\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(self.apply_edges)\n",
        "            return g.edata['score']\n",
        "\n",
        "class DotPredictor(nn.Module):\n",
        "    def forward(self, g, h):\n",
        "        with g.local_scope():\n",
        "            g.ndata['h'] = h\n",
        "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
        "            return g.edata['score'][:, 0]\n",
        "\n",
        "def compute_loss(pos_score, neg_score):\n",
        "    scores = torch.cat([pos_score, neg_score])\n",
        "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
        "    return F.binary_cross_entropy_with_logits(scores, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zCYeobvS60vr"
      },
      "outputs": [],
      "source": [
        "def NN(train_edges=train_edges,\n",
        "       unlabeled_edges=unlabeled_edges,\n",
        "       test_size=0.1,\n",
        "       seed=42,\n",
        "       num_epochs=1000,\n",
        "       lr=3e-4,\n",
        "       print_train=True,\n",
        "       name_model='GCN'):\n",
        "  \n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    g = dgl.DGLGraph().to(device)\n",
        "    g.add_edges(train_edges[:, 0], train_edges[:, 1])\n",
        "    g.ndata['feat'] = torch.Tensor(node_feat)\n",
        "\n",
        "    u, v = g.edges()\n",
        "\n",
        "    eids = np.arange(g.number_of_edges())\n",
        "    eids = np.random.permutation(eids)\n",
        "    test_size = int(len(eids) * test_size)\n",
        "    train_size = g.number_of_edges() - test_size\n",
        "    test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
        "    train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
        "\n",
        "    neg_v = np.random.randint(NUM_NODES, size=len(train_edges))\n",
        "\n",
        "    neg_eids = np.random.choice(len(neg_v), g.number_of_edges())\n",
        "    test_neg_u, test_neg_v = test_pos_u, neg_v[neg_eids[:test_size]]\n",
        "    train_neg_u, train_neg_v = train_pos_u, neg_v[neg_eids[test_size:]]\n",
        "\n",
        "    train_g = dgl.remove_edges(g, eids[:test_size]).to(device)\n",
        "    train_g = dgl.add_self_loop(train_g)\n",
        "\n",
        "    train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes()).to(device)\n",
        "    train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes()).to(device)\n",
        "\n",
        "    test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes()).to(device)\n",
        "    test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes()).to(device)\n",
        "\n",
        "    if name_model == 'GCN':\n",
        "        model = GCN(train_g.ndata['feat'].shape[1], 32)\n",
        "    elif name_model == 'GAT':\n",
        "        model = GAT(train_g.ndata['feat'].shape[1], 32)\n",
        "    elif name_model == 'GraphSAGE':\n",
        "        model = GraphSAGE(train_g.ndata['feat'].shape[1], 32)\n",
        "\n",
        "    #pred = MLPPredictor(8)\n",
        "    pred = DotPredictor()\n",
        "    optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200, 400, 500, 600, 800], gamma=0.5)\n",
        "    all_logits = []\n",
        "\n",
        "    best_model = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "        h = model(train_g, train_g.ndata['feat'])\n",
        "        pos_score = pred(train_pos_g, h)\n",
        "        neg_score = pred(train_neg_g, h)\n",
        "        loss = compute_loss(pos_score, neg_score)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pos_score = pred(test_pos_g, h)\n",
        "            neg_score = pred(test_neg_g, h)\n",
        "            loss_test = compute_loss(pos_score, neg_score)\n",
        "            if loss_test < best_loss:\n",
        "                best_model = copy.deepcopy(model.state_dict())\n",
        "                best_loss = loss_test\n",
        "           \n",
        "        if e % (num_epochs // 20) == 0 and print_train:\n",
        "            print('In epoch {}, loss: {}, test_loss: {}'.format(e, loss, loss_test))\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "    test_g = dgl.DGLGraph().to(device)\n",
        "    test_g.add_edges(unlabeled_edges[:, 0], unlabeled_edges[:, 1])\n",
        "    test_g.ndata['feat'] = torch.Tensor(node_feat)\n",
        "    test_g_ = dgl.add_self_loop(test_g).to(device)\n",
        "    h = model(test_g_, test_g_.ndata['feat'])\n",
        "    pos_score = pred(test_g, h)\n",
        "    test_prediction = np.array(torch.sigmoid(pos_score) >= 0.5, dtype=np.int64)\n",
        "\n",
        "    return test_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZcpMbT98aAA",
        "outputId": "9591c783-bb9c-4441-b74e-a5cc03dda478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In epoch 0, loss: 0.6820926666259766, test_loss: 0.7170300483703613\n",
            "In epoch 50, loss: 0.6568458080291748, test_loss: 0.6699837446212769\n",
            "In epoch 100, loss: 0.6359355449676514, test_loss: 0.6556169986724854\n",
            "In epoch 150, loss: 0.6174577474594116, test_loss: 0.6488596200942993\n",
            "In epoch 200, loss: 0.6015124917030334, test_loss: 0.6379255056381226\n",
            "In epoch 250, loss: 0.5922828912734985, test_loss: 0.6305621266365051\n",
            "In epoch 300, loss: 0.5833888053894043, test_loss: 0.6242129802703857\n",
            "In epoch 350, loss: 0.5757639408111572, test_loss: 0.6199895143508911\n",
            "In epoch 400, loss: 0.5694897174835205, test_loss: 0.617525577545166\n",
            "In epoch 450, loss: 0.5667153000831604, test_loss: 0.616649329662323\n",
            "In epoch 500, loss: 0.5640900731086731, test_loss: 0.6159230470657349\n",
            "In epoch 550, loss: 0.5628214478492737, test_loss: 0.6156011819839478\n",
            "In epoch 600, loss: 0.5615806579589844, test_loss: 0.6153140068054199\n",
            "In epoch 650, loss: 0.5609689950942993, test_loss: 0.6151817440986633\n",
            "In epoch 700, loss: 0.5603627562522888, test_loss: 0.615058958530426\n",
            "In epoch 750, loss: 0.5597609877586365, test_loss: 0.614947497844696\n",
            "In epoch 800, loss: 0.5591631531715393, test_loss: 0.6148399114608765\n",
            "In epoch 850, loss: 0.5588651299476624, test_loss: 0.6147871613502502\n",
            "In epoch 900, loss: 0.5585678219795227, test_loss: 0.6147378087043762\n",
            "In epoch 950, loss: 0.5582709312438965, test_loss: 0.6146922707557678\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "NN()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_array = (0, 1, 42)\n",
        "NN_predictions = {\n",
        "    'GCN': [],\n",
        "    'GAT': [],\n",
        "    'GraphSAGE': [],\n",
        "}"
      ],
      "metadata": {
        "id": "X5nfyFUKvqYn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for seed in seed_array:\n",
        "    for name_model in NN_predictions.keys():\n",
        "        test_prediction = NN(\n",
        "            seed=seed,\n",
        "            name_model=name_model,\n",
        "            print_train=False,\n",
        "            test_size=0.01)\n",
        "        print(f'seed = {seed}, \\t type = {name_model}')\n",
        "        NN_predictions[name_model].append(test_prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qd-R1nJvzOq",
        "outputId": "566fc743-7266-4f61-dfd6-56c2bac9090a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed = 0, \t type = GCN\n",
            "seed = 0, \t type = GAT\n",
            "seed = 0, \t type = GraphSAGE\n",
            "seed = 1, \t type = GCN\n",
            "seed = 1, \t type = GAT\n",
            "seed = 1, \t type = GraphSAGE\n",
            "seed = 42, \t type = GCN\n",
            "seed = 42, \t type = GAT\n",
            "seed = 42, \t type = GraphSAGE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pred = []\n",
        "for name, pr in NN_predictions.items():\n",
        "    for arr in pr:\n",
        "        all_pred.append(arr)"
      ],
      "metadata": {
        "id": "yccXRQeVwEMS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG6E_BuSQ40v",
        "outputId": "eb7ee3cd-3862-4360-ae81-bc131cb3b94c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40206"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "nn_mean_predict = np.array(np.array(all_pred).mean(axis=0) > 0.5, dtype=np.int64)\n",
        "nn_mean_predict.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Слегка ужесточим трешхолд"
      ],
      "metadata": {
        "id": "3UqInWWrdkqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_mean_predict = np.array(np.array(all_pred).mean(axis=0) > 0.9, dtype=np.int64)\n",
        "nn_mean_predict.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoSEHv-hwFU7",
        "outputId": "472e1eba-9d64-4734-bf50-62253129f2eb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19704"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('submit_nn.txt', nn_mean_predict, fmt='%i', delimiter='\\n')"
      ],
      "metadata": {
        "id": "E9UMvdVIwFvh"
      },
      "execution_count": 54,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "competition3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}