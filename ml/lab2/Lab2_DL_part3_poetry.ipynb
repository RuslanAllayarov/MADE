{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Lab2_DL_part3_poetry.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjS00yyes9EE"
      },
      "source": [
        "## Lab 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaXETH8Qs9EK"
      },
      "source": [
        "### Part 3. Poetry generation\n",
        "\n",
        "Let's try to generate some poetry using RNNs. \n",
        "\n",
        "You have several choices here: \n",
        "\n",
        "* The Shakespeare sonnets, file `sonnets.txt` available in the notebook directory.\n",
        "\n",
        "* Роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина. В предобработанном виде доступен по [ссылке](https://github.com/attatrol/data_sources/blob/master/onegin.txt).\n",
        "\n",
        "* Some other text source, if it will be approved by the course staff.\n",
        "\n",
        "Text generation can be designed in several steps:\n",
        "    \n",
        "1. Data loading.\n",
        "2. Dictionary generation.\n",
        "3. Data preprocessing.\n",
        "4. Model (neural network) training.\n",
        "5. Text generation (model evaluation).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TkvMPcbs9EM"
      },
      "source": [
        "from random import sample\n",
        "from IPython.display import clear_output\n",
        "from pprint import pprint\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2CcjqsenTGd"
      },
      "source": [
        "Comment: the built models can work with both datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEVifYQ7s9EO"
      },
      "source": [
        "### Data loading: Shakespeare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRHrDKkds9EP"
      },
      "source": [
        "Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`). Simple preprocessing is already done for you in the next cell: all technical info is dropped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-SrHL-vs9EQ"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U32kWH8J_cBg",
        "outputId": "b08b35fe-f9d8-47e5-893a-47c01e3972b5"
      },
      "source": [
        "if not os.path.exists('sonnets.txt'):\n",
        "    !wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/21f_basic/homeworks_basic/lab02_deep_learning/sonnets.txt\n",
        "\n",
        "with open('sonnets.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "    \n",
        "TEXT_START = 45\n",
        "TEXT_END = -368\n",
        "text = text[TEXT_START : TEXT_END]\n",
        "assert len(text) == 2616\n",
        "\n",
        "text = [x.lower() for x in text]\n",
        "text = ''.join(text)\n",
        "\n",
        "assert len(text) == 100225, 'Are you sure you have concatenated all the strings?'\n",
        "assert not any([x in set(text) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
        "print('OK!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-20 03:36:25--  https://raw.githubusercontent.com/girafe-ai/ml-mipt/21f_basic/homeworks_basic/lab02_deep_learning/sonnets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 119748 (117K) [text/plain]\n",
            "Saving to: ‘sonnets.txt’\n",
            "\n",
            "\rsonnets.txt           0%[                    ]       0  --.-KB/s               \rsonnets.txt         100%[===================>] 116.94K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-12-20 03:36:25 (5.56 MB/s) - ‘sonnets.txt’ saved [119748/119748]\n",
            "\n",
            "OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRilrUF0s9ES"
      },
      "source": [
        "### Data loading: \"Евгений Онегин\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcivB9rTs9EU",
        "outputId": "04c84ae4-17af-4f7e-dc35-c9600e9f52ed"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/attatrol/data_sources/master/onegin.txt\n",
        "    \n",
        "with open('onegin.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "    \n",
        "text = [x.replace('\\t\\t', '') for x in text]\n",
        "\n",
        "text = [x.lower() for x in text]\n",
        "text = ''.join(text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-20 05:35:40--  https://raw.githubusercontent.com/attatrol/data_sources/master/onegin.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 262521 (256K) [text/plain]\n",
            "Saving to: ‘onegin.txt.4’\n",
            "\n",
            "\ronegin.txt.4          0%[                    ]       0  --.-KB/s               \ronegin.txt.4        100%[===================>] 256.37K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-12-20 05:35:40 (11.0 MB/s) - ‘onegin.txt.4’ saved [262521/262521]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDihvo50s9EW"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rubTzdUzs9EX"
      },
      "source": [
        "Put all the characters, that you've seen in the text, into variable `tokens`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfZEiOuws9EY"
      },
      "source": [
        "Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "KlGwUKLAs9EY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e477273-34b3-4aaa-873d-87ebbd0c1043"
      },
      "source": [
        "tokens = sorted(set(text))\n",
        "tokens.append('|')\n",
        "\n",
        "token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
        "idx_to_token = {idx: token for token, idx in token_to_idx.items()}\n",
        "\n",
        "def to_matrix(data, max_len=None, pad=token_to_idx['|'], dtype='int32', batch_first = True):\n",
        "\n",
        "    max_len = max_len or max(map(len, data))\n",
        "    data_ix = np.zeros([len(data), max_len], dtype) + pad\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        line_ix = [token_to_idx[c] for c in data[i]]\n",
        "        data_ix[i, :len(line_ix)] = line_ix\n",
        "        \n",
        "    if not batch_first:\n",
        "        data_ix = np.transpose(data_ix)\n",
        "\n",
        "    return data_ix\n",
        "\n",
        "train_dataset = []\n",
        "for x in text.split('\\n'):\n",
        "  if len(x) > 10:\n",
        "    train_dataset.append(x)\n",
        "\n",
        "print(f'Size of train dataset: {len(train_dataset)} \\n')\n",
        "\n",
        "print('\\n'.join(train_dataset[:5]))\n",
        "print(to_matrix(train_dataset[:5]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train dataset: 5254 \n",
            "\n",
            "«мой дядя самых честных правил,\n",
            "когда не в шутку занемог,\n",
            "он уважать себя заставил\n",
            "и лучше выдумать не мог.\n",
            "его пример другим наука;\n",
            "[[43 57 59 54  1 49 76 49 76  1 62 45 57 72 66  1 68 50 62 63 58 72 66  1\n",
            "  60 61 45 47 53 56  5]\n",
            " [55 59 48 49 45  1 58 50  1 47  1 69 64 63 55 64  1 52 45 58 50 57 59 48\n",
            "   5 83 83 83 83 83 83]\n",
            " [59 58  1 64 47 45 51 45 63 73  1 62 50 46 76  1 52 45 62 63 45 47 53 56\n",
            "  83 83 83 83 83 83 83]\n",
            " [53  1 56 64 68 69 50  1 47 72 49 64 57 45 63 73  1 58 50  1 57 59 48  7\n",
            "  83 83 83 83 83 83 83]\n",
            " [50 48 59  1 60 61 53 57 50 61  1 49 61 64 48 53 57  1 58 45 64 55 45 13\n",
            "  83 83 83 83 83 83 83]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwajcAjbs9EZ"
      },
      "source": [
        "*Comment: in this task we have only 38 different tokens, so let's use one-hot encoding.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAM0kqlVs9Ea"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDV2e9XGs9Ea"
      },
      "source": [
        "Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.\n",
        "\n",
        "Let's use vanilla RNN, similar to the one created during the lesson."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW_qd2TU1CNk"
      },
      "source": [
        "class CharRNNCell(nn.Module):\n",
        "\n",
        "    def __init__(self, num_tokens=len(tokens), embedding_size=64, rnn_num_units=256):\n",
        "        super(self.__class__,self).__init__()\n",
        "        self.num_units = rnn_num_units\n",
        "        self.num_tokens = num_tokens\n",
        "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
        "        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n",
        "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        \n",
        "    def forward(self, x, h_prev):\n",
        "        x_emb = self.embedding(x)\n",
        "        x_and_h = torch.cat([x_emb, h_prev], dim=-1)\n",
        "        h_next = self.rnn_update(x_and_h)\n",
        "        \n",
        "        h_next = torch.tanh(h_next)\n",
        "        \n",
        "        logits = self.rnn_to_logits(h_next)\n",
        "        \n",
        "        return h_next, F.log_softmax(logits, -1)\n",
        "    \n",
        "    def initial_state(self, batch_size):\n",
        "        return torch.zeros(batch_size, self.num_units, requires_grad=True)\n",
        "\n",
        "    def rnn_loop(self, batch_ix):\n",
        "        batch_size, max_length = batch_ix.size()\n",
        "        hid_state = self.initial_state(batch_size)\n",
        "        logprobs = []\n",
        "\n",
        "        for x_t in batch_ix.transpose(0,1):\n",
        "            hid_state, logp_next = self.forward(x_t, hid_state)\n",
        "            logprobs.append(logp_next)\n",
        "        \n",
        "        return torch.stack(logprobs, dim=1)\n",
        "\n",
        "    def fit(self, train_dataset, num_iter=1000, plot_loss=False):\n",
        "        self.MAX_LENGTH = max(map(len, train_dataset))\n",
        "        opt = torch.optim.Adam(self.parameters())\n",
        "        history = []\n",
        "\n",
        "        for i in range(num_iter):\n",
        "            batch_ix = to_matrix(sample(train_dataset, 32), max_len=self.MAX_LENGTH)\n",
        "            batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "            \n",
        "            logp_seq = self.rnn_loop(batch_ix)\n",
        "            \n",
        "            predictions_logp = logp_seq[:, :-1]\n",
        "            actual_next_tokens = batch_ix[:, 1:]\n",
        "\n",
        "            loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            \n",
        "            if plot_loss:\n",
        "              history.append(loss.data.numpy())\n",
        "              if (i+1)%(num_iter//100)==0:\n",
        "                  clear_output(True)\n",
        "                  plt.plot(history,label='loss')\n",
        "                  plt.title('Fit CharRNNCell')\n",
        "                  plt.legend()\n",
        "                  plt.show()\n",
        "                  print(f'loss: {loss.data.numpy()}')\n",
        "\n",
        "    def generate_sample(self, seed_phrase=' ', max_length=None, temperature=1.0):\n",
        "        assert self.MAX_LENGTH, \"You should fit model before generate sample\"\n",
        "        max_length = max_length if max_length else self.MAX_LENGTH\n",
        "\n",
        "        x_sequence = [token_to_idx[token.lower()] for token in seed_phrase]\n",
        "        x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
        "        hid_state = self.initial_state(batch_size=1)\n",
        "        \n",
        "        for i in range(len(seed_phrase) - 1):\n",
        "            hid_state, _ = self.forward(x_sequence[:, i], hid_state)\n",
        "\n",
        "        for _ in range(max_length - len(seed_phrase)):\n",
        "            hid_state, logp_next = self.forward(x_sequence[:, -1], hid_state)\n",
        "            p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
        "\n",
        "            next_ix = np.random.choice(self.num_tokens, p=p_next)\n",
        "            next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
        "            x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
        "            \n",
        "        return ''.join([idx_to_token[ix] for ix in x_sequence.data.numpy()[0]]).split('|')[0]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1FIztzds9Ec"
      },
      "source": [
        "Plot the loss function (axis X: number of epochs, axis Y: loss function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "0X7y0Zgds9Ec",
        "outputId": "56ad3756-6750-407a-e992-5615605b6036"
      },
      "source": [
        "char_rnn_cell = CharRNNCell()\n",
        "char_rnn_cell.fit(train_dataset, num_iter=4000, plot_loss=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.26812446117401123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyrd2HHP3Zad",
        "outputId": "9daa95ab-0b11-4be8-e927-83b150817256"
      },
      "source": [
        "for t in [0.1, 0.2, 0.5, 1.0, 2.0]:\n",
        "  print(f\"temperature = {t}:\")\n",
        "  for _ in range(3):\n",
        "    print('\\t', char_rnn_cell.generate_sample(temperature=t))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temperature = 0.1:\n",
            "\t  такой в сада молодой,\n",
            "\t  нет был первый картиной страда,\n",
            "\t  него не нас очень мил.\n",
            "temperature = 0.2:\n",
            "\t  нет часто в саду полной\n",
            "\t  не вот он очень меж терев,\n",
            "\t  невольно в перед ней нежной\n",
            "temperature = 0.5:\n",
            "\t  это бы в первых муж тот;\n",
            "\t  татьяна в первый разделой\n",
            "\t  вердечно, в как сердца порадей\n",
            "temperature = 1.0:\n",
            "\t  ней печалье напевет —\n",
            "\t  жесно высогою муток.\n",
            "\t  бегот жажен разврагем:\n",
            "temperature = 2.0:\n",
            "\t  пчет бщаям; трифлуедезгу.\n",
            "\t  оро ввал, сей, скровопотные,:\n",
            "\t  фмилечкй, ннзиглихом еmу.),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DbXjsTRs9Ed"
      },
      "source": [
        "### More poetic model\n",
        "\n",
        "Let's use LSTM instead of vanilla RNN and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78e3Zf4ps9Ee"
      },
      "source": [
        "Plot the loss function of the number of epochs. Does the final loss become better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xo5AYGK8s9Ee"
      },
      "source": [
        "class CharRNNLoop(nn.Module):\n",
        "    def __init__(self, num_tokens=len(tokens), emb_size=64, rnn_num_units=256):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.num_tokens = num_tokens\n",
        "        self.MAX_LENGTH = None\n",
        "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
        "        self.rnn = nn.LSTM(emb_size, rnn_num_units, batch_first=True)\n",
        "        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        self.rnn_num_units = rnn_num_units\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_seq, _ = self.rnn(self.emb(x))\n",
        "        next_logits = self.hid_to_logits(h_seq)\n",
        "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
        "        return next_logp\n",
        "\n",
        "    def fit(self, train_dataset, num_iter=1000, plot_loss=False):\n",
        "        self.MAX_LENGTH = max(map(len, train_dataset))\n",
        "        opt = torch.optim.Adam(self.parameters())\n",
        "        history = []\n",
        "\n",
        "        for i in range(num_iter):\n",
        "            batch_ix = to_matrix(sample(train_dataset, 32), max_len=self.MAX_LENGTH)\n",
        "            batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "            \n",
        "            logp_seq = self.forward(batch_ix)\n",
        "            \n",
        "            predictions_logp = logp_seq[:, :-1]\n",
        "            actual_next_tokens = batch_ix[:, 1:]\n",
        "\n",
        "            loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))\n",
        "            \n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            \n",
        "            history.append(loss.data.numpy())\n",
        "            if plot_loss:\n",
        "              if (i+1)%(num_iter//100)==0:\n",
        "                  clear_output(True)\n",
        "                  plt.plot(history,label='loss')\n",
        "                  plt.title('Fit CharRNNLoop')\n",
        "                  plt.legend()\n",
        "                  plt.show()\n",
        "                  print(f'loss: {loss.data.numpy()}')\n",
        "\n",
        "    def generate_sample(self, seed_phrase=' ', max_length=None, temperature=1.0):\n",
        "        assert self.MAX_LENGTH, \"You should fit model before generate sample\"\n",
        "        max_length = max_length if max_length else self.MAX_LENGTH\n",
        "\n",
        "        x_sequence = [token_to_idx[token.lower()] for token in seed_phrase]\n",
        "        x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
        "        \n",
        "        logp_next = self.forward(x_sequence[:, :len(seed_phrase)])\n",
        "\n",
        "        for _ in range(max_length - len(seed_phrase)):\n",
        "            logp_next = self.forward(x_sequence[:, :])\n",
        "            p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
        "            next_ix = np.random.choice(self.num_tokens, p=p_next[-1, :])\n",
        "            next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
        "            x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
        "            \n",
        "        return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]]).split('|')[0]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Pg-6NoXMOW1t",
        "outputId": "df3e8d2f-bb9b-4187-c9e6-48643879dcef"
      },
      "source": [
        "char_rnn_loop = CharRNNLoop()\n",
        "char_rnn_loop.fit(train_dataset, num_iter=4000, plot_loss=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.22776798903942108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx_EFDyamnls"
      },
      "source": [
        "Answer: Yes, loss become better (Cell ~ 0.27, Loop ~ 0.23)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC0tpXlLs9Ee"
      },
      "source": [
        "Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.\n",
        "\n",
        "Evaluate the results visually, try to interpret them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVwsECIDs9Ee",
        "outputId": "284482a6-e4bf-4dfc-94b2-0194c981ebdf"
      },
      "source": [
        "for t in [0.1, 0.2, 0.5, 1.0, 2.0]:\n",
        "  print(f\"temperature = {t}:\")\n",
        "  for _ in range(3):\n",
        "    print('\\t', char_rnn_loop.generate_sample(temperature=t))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temperature = 0.1:\n",
            "\t  не так ли было, и все грустья,\n",
            "\t  не потомушным обезами,\n",
            "\t  не так и страсть и под ней поря,\n",
            "temperature = 0.2:\n",
            "\t  не там в гором он всегда сон,\n",
            "\t  не простой и в самом душей.\n",
            "\t  не постели в том не видит;\n",
            "temperature = 0.5:\n",
            "\t  но нашей столь ее разговой,\n",
            "\t  страшной кажете не замети,\n",
            "\t  любви дели дом не открылась:\n",
            "temperature = 1.0:\n",
            "\t  не скучал тишин чисть прелест,\n",
            "\t  bсидет она всё гостей поткой\n",
            "\t  усоб вер, не она с блестит.\n",
            "temperature = 2.0:\n",
            "\t  оглпет. хокшу-he\n",
            "\t  .rz\n",
            "ымец ищумни, сросце.\n",
            "\t  чарой пусдивцомых с»льi?.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "janOUa0cmrtb"
      },
      "source": [
        "Answer: the lower the temperature, the more the model generates popular combinations of symbols. The higher the temperature, the more diverse characters are selected, but the text turns out to be less meaningful. The ideal temperature is around 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mgVLfj6s9Ef"
      },
      "source": [
        "### Saving and loading models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSXE8PZZs9Ef"
      },
      "source": [
        "Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7weWD72s9Ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa2bae07-baa4-4b84-f73d-b384a2e54f8d"
      },
      "source": [
        "torch.save(char_rnn_loop, 'model')\n",
        "model = torch.load('model')\n",
        "model.generate_sample(temperature=0.5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' но тайный все умеренный'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSO8ue_js9Ef"
      },
      "source": [
        "### References\n",
        "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a> \n",
        "There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.\n",
        "2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>\n",
        "3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch`)"
      ]
    }
  ]
}